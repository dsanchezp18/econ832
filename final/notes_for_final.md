# Notes for the final exam

The final exam will be about this dataset and this choice prediction task

- https://cpc-18.com/

Read the website and see the papers models and codes

## Notes about the Choice Prediction Competition 2018

*Behavioral decision research highlights interesting choice anomalies and proposes elegant models that can explain these phenomena. Thus, it seems reasonable that this type of research would provide a solid base for development of useful predictive models of choice. In practice, however, it is often easier to predict behavior with data-driven machine learning tools that do not rely on the behavioral decision-making literature.*

- ML better than behavioural science in predicting choice.

- One reason for the difficulty in deriving general predictions using the elegant behavioral models is that different models are often proposed to explain different phenomena.

- The CPC18 data used to train and test the predictive models of choice comes from the same experimental paradigm used in CPC15 (Erev et al., 2017). In this paradigm, decision makers are faced with descriptions of two monetary prospects and are asked to choose between them repeatedly for 25 trials. After each of the first five trials, decision makers do not receive feedback on their choice. After each trial thereafter (i.e. starting Trial 6), decision makers get full feedback concerning the outcomes generated by each option in that trial (both the obtained payoff and the forgone payoff are revealed).

## CPC18 Data Description

Models competing in the current choice prediction competition can make use of the data collected as part of CPC15 (Erev et al., 2017). These data include Problems 1-150 given in the link (Estimation Data) below. In addition, to supplement and extend the data (and the space of choice problems), we ran Experiment 1, and plan to run Experiment 2 at a later date. The data of the latter will be used as the competition set for one of the tracks of the choice prediction competition (see below). The apparatus and design of these two new experiments are very similar to those used in Erev et al. (2017) and the reader is referred there for more details. Hereinafter, we provide the essentials.

### Experiment 1 data

- Different in that includes full feedback after each trial.

- Experiment 1 includes Problems 151-210 listed in the link (Estimation data) below. These 60 choice problems were randomly selected according to the problem selection algorithm which is detailed here. Two-hundred and forty participants (139 female, MAge = 24.5) participated in the experiment, half at the Technion and half at the Hebrew University of Jerusalem. 

- Each participant faced one set of 30 problems: half faced Problems 151-180 and the other half faced problems 181-210 (360 probelms, GameID variable). (Sets). The order of the problems was randomized among participants.

- Each problem was faced for 25 trials consecutively. The first five trials without feedback and the rest with full feedback. 

- Participants were paid for one randomly selected choice they made, in addition to a show-up fee. The final payoff ranged between 10 and 136 shekels, with a mean of 40 (about 11 USD).